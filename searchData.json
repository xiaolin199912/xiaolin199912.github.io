[{"title":"【论文翻译】GPEN:基于先验GAN嵌入网络的盲人脸重建问题解决方案","url":"/2021/05/27/【论文翻译】GPEN/","content":"\n![1](https://linimages.oss-cn-beijing.aliyuncs.com/typora/header_image2.png)\n\n## 1～2.开门见山\n\n​\t在这个工作中，我们重新探索了盲人脸超分辨问题。我们的目标是复原那些在自然界观察得到的退化人脸，我们的idea是去无缝融合GAN和DNN的优点。我们首先预训练了一个GAN，这个GAN是为了高清人脸照片生成的，以及把它嵌入到一个DNN中作为一个人脸修复的前置解码器。\n\n​\t然后，这个以GAN为前置解码器的DNN被一系列的综合的一对对低情-高清人脸微调。在此期间DNN学习将退化的图像输入映射到一个所需的潜在空间，以至于这个GAN前置网络可以重建所需的高清人脸照片。\n\n​\t我们仔细地设计了GAN模块，让它能更好与符合一个U形设计的DNN。在这里这些深层特征是针对全部人脸重建提取的，被用于生成浅编码，同时浅层特征被用于作为噪声去生成局部人脸细节和保持图像背景。\n\n​\t在这个方法下，我们训练好的模型可以重建高清人脸的真实照片细节从自然界的退化人脸，避免了过度平滑的结果（由盲人脸问题的high illness造成的）。\n\n做个总结：\n\n- 我们学习和嵌入了一个GAN前置网络到一个DNN中，并且为了在自然盲人脸重建中有效，我们微调了这个被嵌入到的DNN中的GAN（？）。这是有价值的记录了之前仅将预训练的GAN转化为一个网络而没有微调。\n- 这个GAN模块被设计过，所以他们可以被轻松地嵌入到一个U形的DNN中通过微调。GAN的浅编码和噪声输入是分别被生成从DNN的深层和浅层特征来重建全局结构，局部人脸细节和图像背景。\n- 我们的模型设置了新的SOTA在盲人脸复原，这非常好的解决了真实世界退化照片复原问题。\n\n## 3.1动机与框架\n\n​\tBFR是一个经典的ill-posed inverse问题。X空间里是退化低清人脸，Y空间是原始高清人脸，给一个低清人脸输入x*∈* X，BFR目标是找到它合适的清晰人脸图像y*∈* Y。\n\n​\t大多数基于DNN的办法学习一个匹配功能Φ 去达到目的。Φ (x)—>y。但是，这是一个一对多的逆问题，在Y中有很多可能的人脸图能匹配到输入x。通常存在办法训练DNN模型是建立x与y之间的映射，使用一些逐像素的loss函数。结果，正如我们在下面看到的那样，最后的结果Φ(x)倾向于成为那些HQ人脸的平均，它是过度平滑且失去了细节。\n\n![image-20210527214539138](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527214539138.png)\n\n​\t这符合视觉感知全局优先理论。\n\n​\tcGAN方法可以部分的解决这个问题通过对抗训练去减少在匹配中的不确定性。但是，当退化是严重时，这个问题仍然存在，并且cGANs几乎不能生成清晰的带有真实纹理和细节的人脸图像。\n\n​\t与这之前的办法不同的是，我们首先训练一个GAN先验网络，然后把它嵌入到一个DNN中作为一个针对高清人脸复原的解码器。我们叫他们的算法先验GAN嵌入网络（GPEN）。我们的GPEN的第一部分是一个CNN编码器，它学习到将输入的退化图像x映射到一个需要的隐编码z在GAN的隐空间Z中。\n\n​\t然后GAN先验网络能够重产出需要的高清人脸图像通过G(z)—>y，在这里G指的是GAN的预训练生成器。生成过程基本是一个点到点的映射，能极大地减轻在过去办法中的一到多映射的不确定性。GAN拟方向办法分享了一个相似的idea给我们的GPEN。但是，他们保持了与训练GANs不改变为了人脸操作的一致和方便。\n\n​\t而在GPEN中，我们仔细地设计和预训练了GAN模块和微调了先验GANs为了有效的BFR。GPEN的结构和GAN模块如下，它们将在下面的sections详细解释。\n\n## 3.2网络结构\n\n![image-20210527223945663](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527223945663.png)\n\n### GAN先验网络\n\n​\tU-Net已经成功地且广泛地在很多图像复原任务中被使用，它已经证明了在复原图像细节上的有效性。因此，我们的GPEN全部是一个U形的编码器-解码器结构。依此，GAN先验网络应该被设计成去满足两个需求：\n\n（1）它能很好的生成高清人脸图像\n\n（2）它可以容易地作为一个解码器嵌入到U形GPEN中。\n\n​\t受到SOTA的GAN结构StyleGAN的启发，我们使用一个映射网络去规划隐编码z进入一个更少参数的纠缠空间（entangled space）w*∈*W。这个中间编码w是被投射到每个GAN模块中。\n\n​\t因为GAN先验网络将会被嵌入到U形DNN中进行微调，我们需要留出空间给U形DNN的编码器提取的跳过特征图。因此我们提出增加noise输入到每个GAN模块。\n\n​\t对于GAN模块结构，有几个选项。在这个工作中，我们采用的是StyleGAN v2的结构，因为它对于生成高清图像的强大能力。GAN模块的数量和跳过U形DNN提取的特征图的数量是相等的，这与输入人脸图像的分辨率有关。\n\n​\tStyleGAN需要两个不同的noise输入在每个GAN模块中。为了使GAN先验网络容易地嵌入到U形GPEN中，不同于StyleGAN，noise输入是在相同的空间分辨率被复用的对所有GAN模块。\n\n### 完整的网络结构\n\n​\t一旦GAN先验网路被训练通过使用一些数据机（比如FFHQ），我们把它嵌入到U形DNN中作为一个解码器。隐编码z和输入到GAN模块的noise被全连接层（深度特征）的输入和DNN编码器的shallower层分别取代，这将控制全局人脸结构、局部人脸细节和人脸图像背景的重建。\n\n​\t因为提出的模型不是全卷积，所以低清人脸图像首先被resize到需要的尺寸（1024*1024），使用简单的BICUBIC在输入到GPEN之前。在嵌入后，整个GPEN将会被微调以至于编码部分和解码部分可以学习去彼此适应。\n\n​\t\n\n## 3.3 训练策略\n\n​\t我们首先使用一个高清人脸数据集预训练GAN先验网络，训练策略和StyleGAN一致。这个预训练GAN模型被嵌入到GPEN中，并且我们使用一系列的人工合成LQ-HQ人脸图像对来微调整个网络（人工合成流程在4.2）。\n\n​\t为了微调GPEN模型，我们采用三个loss函数：adversarial loss、content loss、feature matching loss。\n\n- La：被GAN先验网络继承来的：\n\n  ![image-20210527231935154](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527231935154.png)\n\n​\t其中X和X～表示ground-truth高清图像和退化LQ图像，G是训练中的生成器，D是鉴别器。\n\n- Lc被定义为在生成器的最终结果和匹配的原始图像的L1范数距离。\n- Lf与感知损失相似，但是它基于分类器而不是预训练的VGG网络，这更符合我们的工作。它被如下构建：\n\n![image-20210527232341867](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527232341867.png)\n\n​\t在这里T是中间层（被用于特征提取）的全部数量。Di(X)是鉴别器D的第i层提取特征。\n\n​\t最终的loss L 定义如下：\n\n![image-20210527232546705](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527232546705.png)\n\n​\t在这里*α* 和 *β*是平衡参数。The content loss Lc 强化了好的特征和保护了原始彩色信息。通过引导Lf在鉴别器中，La会有更好的平衡去复原更真实的人脸图像的丰富细节。在所有的实验中，我们设置*α* =1和 *β*=0.02。  \n\n\n\n## 4.1数据集和评估方案\n\n​\tFFHQ数据机包含了7w张高清人脸图像，分辨率均为1024*1024，被用于训练我们的模型。我们首先使用它去训练GAN先验网络，然后采用基于FFHQ数据集制备的退化数据来微调整个GPEN。\n\n​\t为了评估我们的模型，我们使用CelebA-HQ数据机去评估低清人脸图像来量化比较GPEN与其他的SOTA方案。我们也收集了1000张来自网络的真实世界的低清人脸来评估我们的模型在wild的表现。\n\n​\t在量化评估里，PSNR，FID和LPIPS是被使用的。值得一提的是，所有这些方法只能被作为参考，因为他们不能真正的反应一个盲人脸修复算法的表现，特别是对于wild盲人脸复原问题。\n\n## 4.2低清与高清图像对的制备\n\n​\t我们人工合成（synthesize）退化人脸，这些人脸源于FFHQ中的高清图像，它们使用了下面的退化模型：\n\n![image-20210527180451448](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527180451448.png)\n\n- I：输入人脸图\n- k：模糊核\n- n*σ*：高斯噪声\n- Id：退化图像\n- 圆圈+叉：二维卷积\n- 下箭头+s：标准s折下采样\n- JPEG带有q质量因子的压缩操作\n\n​    以上的退化模型以及被使用在了过去的办法中。\n\n​\t在我们的方案，对于每张图片：\n\n- 模糊核k是从一系列的模糊模型中随机选择的，包括高斯模糊和运动模糊用了变化的核尺寸。\n- 额外的高斯噪声n是从正态分布沿通过取平均值，*σ*是从[0,25]取的。\n- s的值是随机和一律从[10,200]采样（直到200次下采样）。\n- q是对每张图片随机和一律从[5,50]（直到95%JPEG压缩率）\n\n​    通过使用这些退化图像来微调模型，GPEN的编码部分可以学习去生成合适的浅编码和噪声输入，浅编码和噪声输入到GAN解码网络里，这同时被更新去处理现实脚本中的退化人脸。\n\n​\t在模型更新期间，我们采用了Adam优化器带着batchsize=1。学习率是变化的对于GPEN的不同部分，包括编码器，解码器和分类器。在我们的方案中，我们让编码器的学习率=0.002，设置编码器：解码器：分类器=100:10:1。它应该被记录的是，分类器部分在测试环节中将被移除。\n\n\n\n## 4.3 消融实验\n\n​\t为了更好的理解GPEN中不同组件的角色以及训练策略，在这个部分我们做了一个消融实验通过指导GPEN中的一些变体和比较他们的盲人脸复原表现。\n\n​\t第一个变体是GPEN-w/o-ft，它的GAN先验网络在整个微调流程中保持不变。\n\n​\t第二个变体是GPEN-w/o-noise，这指的是这个GPEN模型没有噪声输入。\n\n​\t第三个变体是GPEN-noise-add，这是指噪声输入是add的而不是连接到卷积中的。\n\n​\t我们基于CelebA-HQ数据集来评估GPEN和它的三个变体。LQ图像是使用退化模型制备的（4.2）。\n\n![image-20210527204206871](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527204206871.png)\n\n​\t我们可以看到GPE有更好的数值表现比起它的变体。\n\n![image-20210527204311354](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210527204311354.png)\n\n​\t上图展示了变体与本体在一张图上的不同结果。我们可以看到GPEN-w/o-ft可以生成干净的高清人脸图像，但是，这张人脸和ground-truth完全不同，以及这张图片的背景也完全不同。这是因为没有微调先验GAN，它是非常困难的生成需要的浅编码到隐空间Z，这符合在很多GAN倒置工作中的发现。\n\n​\t通过消除噪声输出，GPEN-w/o-noise的结果是比起GPEN-w/o-ft更模糊的，以及有一些压缩伪影生成在图像的边缘。这表明噪声输入在合成局部细节上起到了重要作用。\n\n​\tGPEN-noise-add达到了和GPEN相近的效果不是轻微的少了一些面部细节，而它生成了一些错误的细节在图像的背景上。\n\n​\t总的来说，GPEN比起它的变体要有更好的表现，表明了拼接的U形结构和我们的训练方案对于盲人脸复原任务的有效性。\n\n\n\n## 要补习的知识点\n\n- GAN基础知识与训练知识\n- U-net\n- StyleGAN","tags":["机器学习"]},{"title":"MIT AI大佬的一天是怎样度过的","url":"/2021/05/25/MIT AI Research的一天/","content":"\n## 时间管理大师Lex Fridman\n\n​\tLex老哥本职工作是MIT的AI研究员，主要研究方向包括自动驾驶，人机交互等领域。他在MIT教授一系列深度学习，自动驾驶，强化学习相关课程。同时他还是一名知名podcast（？）主播，采访过众多大佬，并且他的podcast很高产。当然最让人惊讶的是，他居然还是一名巴西柔术黑带选手，健身爱好者，以及音乐爱好者（弹得一手好钢琴和吉他）。可谓各个技能点都很突出。\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/blog03-1.png\" alt=\"blog03-1\" height=\"400\" />\n\n## Lex的日常\n\n相信有很多人像我一样好奇lex是怎么做到的，lex在一期视频中解答了这个问题：https://www.youtube.com/watch?v=0m3hGZvD-0s\n\n![image-20210525201818236](https://linimages.oss-cn-beijing.aliyuncs.com/typora/image-20210525201818236.png)\n\n这个视频中lex从睡眠、禅、工作、放松、健身、阅读等多个角度讲述了他的一天，非常有借鉴意义。\n\n### 1.睡眠\n\nlex一般保证自己有6-8个小时的睡眠时间，并安利了一下他用的智能床垫Eight Sleep。\n\n### 2.禅\n\n这个非常有意思，lex会在每天醒来之后在脑海里过一遍“口头禅”，有点吾日三省吾身的味道了，甚至让我想到了戒色吧的戒色语录（不是）。\n\nlex每天的禅是这样的：\n\n- 对自我的行为准则要求，比如\n  - **一天之内不能使用社交网络3次，总时间控制在10分钟之内**\n  - 每天必须**锻炼身体**\n  - 控制**健康饮食**\n- 感恩——想象今天可能是自己活在世界上的最后一天，感激生命是多么的神奇\n- 对自己设定的**5年期计划**\n- 短期的计划，比如2021年的期望目标\n- 对今天的计划进行**图景化的设想**\n- 核心原则的反思。包括共情，同理心，正直，以及身体与精神的强健。\n\n### 3.工作模式\n\n接下来是lex一天中的第一个工作Section，时间为4个小时，在这4小时里除了喝水和上厕所lex不会让任何事情打断自己，绝不刷社交网络。lex也介绍了自己的装备，包括**站立办公桌**（我很想要QAQ）和**人体工程学键盘**。\n\n划重点：**这4个小时没有打断，上厕所也绝不刷手机。**\n\n在后续的放松和洗澡之后，lex会开始第二个4小时工作Section。\n\n社交网络确实对一天的效率有很大影响，包括抖音这样的短视频应用。在学生阶段，没有课的一天这样的工作模式完全是可以尝试的。\n\n顺带一提，lex录视频的早上在做TF Lite的工作。\n\n### 4.放松\n\nlex接下来会开始健身：\n\n- 每天至少跑10公里以上，边跑边听背景音乐（白噪音/有声书），大概1小时\n- 短时间的力量训练，10个俯卧撑+5个引体向上/每分钟，持续15-20分钟。\n\n- Hard on the mind, hard on the body but good for the soul（身心疲惫但是灵魂得到了放松）。\n\n### 5.洗澡\n\n跑完步，lex先回想了一下自己跑步听的有声书（《第三帝国的灭亡》）。\n\n然后洗个澡，他会洗一分钟的冷水澡orz，并用背景音乐来卡时间。\n\n这真是有点robot内味儿了，给因为嫌洗澡麻烦而干脆放弃锻炼的我上了一课。\n\n### 6.吃饭\n\nlex采用了**生酮饮食法**。会按照计划吃各种营收食物，另外还会按需要摄入各种钠、镁、钾、鱼油等补充药片，总体的准备+进食时间大约控制在20-30分钟，边吃这些“美味食物”边思考一些问题。\n\n### 7.浅工作模式\n\n浅工作模式下，lex会变得更“自由散漫”一些，一般会进行email的阅读和回复，一些白天工作的收尾，以及podcast编辑等工作，同样持续4小时。如果状态不好，可能会躺下来看YouTube视频，技术文档等。\n\n浅工作模式感觉已经非常硬核了orz。\n\n### 8.阅读\n\n最后lex会花2小时时间来进行阅读，前1小时是论文阅读，这部分需要非常专注和投入。这一天lex读的是GPT-3和一篇神经科学论文，这里他也简短分享了一下读论文的方法，如果只是看一眼被别人的算法设计，实验结果等收获还是比较小的，更重要的是深入思考如何对这篇论文提出更深刻的问题，思考超出这篇论文内容之外的东西。\n\n后1小时lex会读一些小说或者非虚构类的作品。录视频的这天他读的是陀斯妥耶夫斯基。\n\n### 9.入睡\n\n读完书就该睡了，最后在做一遍感恩，life is amazing！\n\n\n\n## 我的收获\n\n看完lex的个人分享我还是相当震撼的，极致的高效和自律，以及如机器人般的精密时间控制，这些都造就了lex的成功。每天刷社交网站、短视频的时间真的相当多，大多数接触的都是无意义的垃圾信息，并且由于长时间缺乏锻炼和不规律作息，已经能明显感觉自己身体状态的下降，以及大脑不如之间灵活。\n\n接下来的时间我希望：\n\n- 每天起床先做禅的思考，吾日三省吾身，一天要有一个好的开始\n- 每天锻炼身体和合理的饮食，先从田径场跑一天以及做10个俯卧撑开始，多吃水果蔬菜和维生素。像古希腊人那样，磨炼身体，锻炼体魄。\n- 拒绝沉迷社交网站，保证4+4+3的工作模式，4+4工作上迭代产品，学习编程语言与开发技术，3中更新社交媒体，写博客、公众号，观看新的科技资讯等。\n- 阅读的2就先把前1读没看完的产品书，后1读一些科技公司成长史、任务传记啥的。\n- 脚踏实地，仰望星空！\n\n\n\n本文参考自:https://zhuanlan.zhihu.com/p/371254789","tags":["成长"]},{"title":"脱胎换骨！焕影一新v1.1.0【更新日志】","url":"/2021/05/23/焕影一新v1.1.0/","content":"\n**![1](https://linimages.oss-cn-beijing.aliyuncs.com/typora/header_image2.png)**\n\n大家好，这里是p图助力官小林。历时一个月，我们发布了超大版本**v1.1.0**，来看看新版本更新了哪些内容吧。\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/doge.webp\" alt=\"doge\" height=100 />\n\n## 新功能:「动态照片」\n\n让静态的照片根据你的想法动起来，听上去是不是有些不可思议？\n\n在「动态照片」里就能让它变成现实。「动态照片」支持选定照片中的**人脸**与驱动它的**模版**，让它做出模版对应的表情（我们提供了8种模版）。\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/02.webp\" alt=\"img\" height=\"300\" />\n\n通过模版驱动，我们可以让玛丽莲·梦露唱《Bad Guy》\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/03.gif\" alt=\"img\" height=\"300\" />\n\n工具和照片已经在你手里，你能用它整蛊创作出什么样的视频呢？\n\n\n\n## 2.1 功能升级 ——「照片修复」\n\n之前的算法对异常模糊的老照片有好的修复效果，但是对大家日常的自拍/他拍来说，效果却有些差强人意。\n\n![04](https://linimages.oss-cn-beijing.aliyuncs.com/typora/04.webp)\n\n这次更新我们全新升级了「照片修复」算法，**希望在清晰的基础上，让照片更自然好看**，这对日常照片有更好的修复效果，一组修复前后对比。\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/05.webp\" alt=\"img\" height=\"400\" />\n\n细节对比\n\n![img](https://linimages.oss-cn-beijing.aliyuncs.com/typora/06.webp)\n\n为了女生更漂亮的照片，我们拼了orz。\n\n\n\n## 2.2 功能升级——「证件照换底」\n\n曾有妹子向小林吐槽：**“你们的边缘抠的不太理想，最后我还是打车去了照相馆，店家PS操作一会儿就换好了。”**\n\n场面一度十分尴尬\n\n**“但是他居然一个底色就要了10块钱，呜呜呜”**，算上车费，妹子哭的更大声了。\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/07.webp\" alt=\"img\" height=\"200\" />\n\n虽然证件照换底色对于PS熟手来说不是难事，但是对于我们这些小白来说，高昂的学习成本就已经劝退成功，所以只好乖乖让商家割韭菜。\n\n\n\n**本次更新中，证件照换底的人像抠图效果，得到了指数级提升**。头发,人脸,服装的边缘抠图效果都得到了优化\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/08.webp\" alt=\"img\" height=\"250\" />\n\n目前支持**红、白、蓝**三色换底，后续将更新更多的底色供大家选择，以及呼声很高的**一键换西服**功能。\n\n\n\n## 3.UI全面升级\n\n本次更新焕影一新的所有界面，都套上了新皮肤。\n\n<img src=\"https://linimages.oss-cn-beijing.aliyuncs.com/typora/09.webp\" alt=\"09\" height=\"300\" />\n\n\n\n## 4.其他更新\n\n- 新增可对比式照片分享\n\n- 新增每日额度/任务额度机制\n\n- 升级「智能抠图」界面\n\n- 修复「意见反馈」bug\n\n- 新增后端接口监控服务\n\n- 部分文案优化\n\n\n\n## 写在最后\n\n焕影一新从零开发到今天遇到了很多困难，我们也从一个菜鸟学生团队，一点点走向专业。\n\nv1.1.0版本是我们的第一个base版本，算是个小小的里程碑，感谢大家一直以来的支持。\n\n我们会继续迭代好它，做一款不简单的AI影像应用。\n\n![](https://inkfishing.oss-cn-hangzhou.aliyuncs.com/%E7%84%95%E5%BD%B1-%E9%80%9A%E7%94%A8%E6%96%87%E4%BB%B6/appcode.jpg)\n\n\n\n## 开发人员\n\n- 产品:小林\n\n- Web开发:意识流、炎魔の王、Nexisato\n\n- 算法研发:小林、Cuny\n\n- 美术运营:凯撒、振闻\n\n\n\n\n\n\n\n","tags":["产品"]},{"title":"Gradio：轻松实现AI算法可视化部署","url":"/2021/05/21/Gradio：轻松实现AI算法可视化部署/","content":"\n**![1](https://inkfishing.oss-cn-hangzhou.aliyuncs.com/blogs/header_image2.png)**\n\n**如何将你的AI算法迅速分享给别人，让对方体验，一直是一件麻烦事儿。**\n\n首先大部分人都是在本地跑代码，让别人使用你的模型，以往有这三种方案：\n\n- 上github\n- 将代码打包或者封装成docker后，用QQ/百度云/U盘传输\n- 学习前后端知识，写个前端界面，买个域名，用flask这样微服务框架快速部署，看情况结合一下内网穿透。\n\n这些方案的问题在于——前两者需要对方会编程会配置环境（还得愿意），**我们的分享对象满足这个条件的寥寥无几**；后者则需要你这个算法工程师升级成全栈，学习前后端开发，**学习成本太高**。\n\n总结起来：**场景不匹配，需求不契合，费时又费力！**\n\n那么有没有更好的解决方案呢？有！它就是我今天要给大家安利的一个python开源库：**Gradio**。\n\n​\t![image-20210521152614971](https://inkfishing.oss-cn-hangzhou.aliyuncs.com/blogs/01/0101.png)\n\nGradio是MIT的开源项目，GitHub 2k+ star。\n\n使用gradio，只需在原有的代码中**增加几行**，就能自动化生成交互式web页面，并支持多种输入输出格式，比如图像分类中的图>>标签，超分辨率中的图>>图等。\n\n同时还支持生成能外部网络访问的链接，**能够迅速让你的朋友，同事体验你的算法**。\n\n总结起来，它的优势有：\n\n- 自动生成页面且**可交互**\n- 改动**几行代码**就能完成\n- 支持自定义多种输入输出\n- 支持生成**可外部访问的链接**进行分享\n\n目前已经有很多优秀的开源项目使用Gradio做demo页面。那么该怎么使用Gradio，让我们一起来玩玩～\n\n\n\n## Get start\n\n### 0.安装Gradio\n\n```bash\npip install gradio\n```\n\n### 1.写个简单的RGB转灰度\n\n```python\nimport gradio as gr\nimport cv2\n\ndef to_black(image):\n    output = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    return output\n\ninterface = gr.Interface(fn=to_black, inputs=\"image\", outputs=\"image\")\ninterface.launch()\n```\n\ngradio的核心是它的`gr.Interface`函数，用来构建可视化界面。\n\n- fn：放你用来处理的函数\n- inputs：写你的输入类型，这里输入的是图像，所以是\"image\"\n- outputs：写你的输出类型，这里输出的是图像，所以是\"image\"\n\n最后我们用`interface.lauch()`把页面一发布，一个本地静态交互页面就完成了！\n\n在浏览器输入`http://127.0.0.1:7860/`,查收你的页面：\n\n![image-20210521152336959](https://inkfishing.oss-cn-hangzhou.aliyuncs.com/blogs/01/0102.png)\n\n上传一张图片，点击「SUBMIT」，大功告成。\n\n![image-20210521152500026](https://inkfishing.oss-cn-hangzhou.aliyuncs.com/blogs/01/0103.png)\n\n对于任何图像处理类的ML代码来说，只要定义好一个图像输入>>模型推理>>返回图片的函数（逻辑和RGB转灰度图本质上没区别），放到`fn`中，就完事了。\n\n​\t\n\n### 2.增加example\n\n我们可以在页面下方添加供用户选择的测试样例。\n\n在`gr.Interface`里的`examples`中放入图片路径，格式为[[路径1],[路径2],...]。\n\n```python\nimport gradio as gr\nimport cv2\n\ndef to_black(image):\n    output = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    return output\n\ninterface = gr.Interface(fn=to_black, inputs=\"image\", outputs=\"image\",\n                        examples=[[\"test.png\"]])\ninterface.launch()\n```\n\n![image-20210521155634673](https://inkfishing.oss-cn-hangzhou.aliyuncs.com/blogs/01/0104.png)\n\n增加example不仅能让你的UI界面更美观，逻辑更完善，也有一些其他意义：\n\n- 比如做了一个图像去噪算法，但是用户手头并没有躁点照片，example能让他更快的体验到效果\n\n当然example建议选择效果最好的图，原因和写论文选best case一样（大雾）。\n\n## 3.创建一个外部访问链接\n\n创建外部访问链接非常简单，只需要`launch(share=True)`即可，在打印信息中会看到你的外部访问链接。\n\n免费用户的链接可以使用24小时，想要长期的话需要在gradio官方购买云服务。\n\n\n\n## 4.升个级:图像分类pytorch+resnet18\n\n```python\nimport gradio as gr\nimport torch\nfrom torchvision import transforms\nimport requests\nfrom PIL import Image\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = Image.fromarray(inp.astype('uint8'), 'RGB')\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n  return {labels[i]: float(prediction[i]) for i in range(1000)}\n\ninputs = gr.inputs.Image()\noutputs = gr.outputs.Label(num_top_classes=3)\ngr.Interface(fn=predict, inputs=inputs, outputs=outputs).launch()\n```\n\n![image-20210521160432207](https://inkfishing.oss-cn-hangzhou.aliyuncs.com/blogs/01/0105.png)\n\n图像分类任务是机器学习经典任务。上面我们在outputs中设置了`gr.outputs.Label(num_top_classes=3)`，这是Gradio中专为图像分类**展示类标**定制的函数.\n\n**修改inputs和outputs的组合，总能找到适合你的算法的那一个。**\n\nps:不得不说imagenet的类标真细\n\n\n\n## 总结\n\nGradio的最大的价值我认为是缩短了算法与应用的距离，人人都能迅速分享与体验项目成果，这不管在同事交流，项目汇报，甚至是同学吹牛（大雾），都性感多了。\n\n同时，作为一个python库，不论是ML，还是数据分析，图像处理，甚至做一个视频转gif的小工具（自媒体老哥表示很有用），都能适用。\n\n**你还在发愁怎么做大作业PPT汇报吗？不如试试Gradio。**\n\n\n\n## 了解更多\n\n- Gradio官网：https://www.gradio.app/\n- Gradio官方文档：https://gradio.app/docs\n\n- 使用Gradio的项目「Anime2Sketch」：https://www.gradio.app/hub/AK391/Anime2Sketch\n\n","tags":["机器学习"]}]